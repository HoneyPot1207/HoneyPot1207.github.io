<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>0209lucky的望远镜</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="0209lucky的望远镜">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="0209lucky的望远镜">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Xiaolin Liu">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="0209lucky的望远镜" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.1.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">0209lucky的望远镜</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-论文复盘" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/12/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%9B%98/" class="article-date">
  <time class="dt-published" datetime="2022-04-11T17:00:12.590Z" itemprop="datePublished">2022-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本周报主要来复盘看过的论文，以及思考为什么论文里要这样做，探索一些因果关系。从为什么的角度来看又发现了一些新问题。</p>
<h3 id="视频方向的论文："><a href="#视频方向的论文：" class="headerlink" title="视频方向的论文："></a>视频方向的论文：</h3><ul>
<li><h5 id="video-depth-estimation：Robust-Consistent-Video-Depth-Estimation"><a href="#video-depth-estimation：Robust-Consistent-Video-Depth-Estimation" class="headerlink" title="video depth estimation：Robust Consistent Video Depth Estimation"></a>video depth estimation：Robust Consistent Video Depth Estimation</h5><p> 在CVD论文上做了改进，改进之处及原因如下：</p>
<ul>
<li><p>预处理：子采样（为了减少计算量）、MASK-RNN（表示动态的像素，减少动态物体对深度计算的干扰）</p>
</li>
<li><p>使用CNN优化深度$\theta_{depth}$时，利用$\theta_{depth}$优化$\theta_{cam}$并交替优化（因为初始化的$\theta_{cam}$并不准确）</p>
</li>
<li><p>双线性插值代替比例系数（由于上一步的初始$\theta_{cam}$不准确，导致深度估计不准确，再导致$\theta_{cam}$不准确。由于$\theta_{cam}$不可能在一开始准确，只能从深度下手，使用双线性插值实现深度图的低频对齐，把深度弄准确，由深度求$\theta_{cam}$的公式是几何方向的，是准确的，深度准确了，就会求得准确的$\theta_{cam}$。）</p>
</li>
<li><p>几何感知深度滤波（是上一步的遗留问题，由于上一步主要解决深度图的低频对齐，对于高频信息的深度细节会比较平滑，因此他们选择具有几何意识的深度感知滤波器使得高频信息的深度更精确。）</p>
<p> 这篇论文主要的目的是想解决CVD论文的不足之处，即CVD论文的深度估计方法需要准确的相机位姿作为输入，而获得相机位姿的SFM方法却并不靠谱。因此其选择先初始化一个$\theta_{cam}$，通过与深度的交替优化来优化相机位姿并进一步优化深度。由于一开始的相机位姿并不准确，会导致后续的一系列优化都出现抖动，因此他们从深度入手，使用双线性插值使深度准确来消除抖动。但这样做会使低频信息对齐而高频信息模糊，为了解决高频信息的问题引入了几何感知深度滤波。</p>
</li>
</ul>
</li>
<li><h5 id="video-inpainting-2"><a href="#video-inpainting-2" class="headerlink" title="video inpainting*2"></a>video inpainting*2</h5><p>①Progressive Temporal Feature Alignment Network for Video Inpainting</p>
<ul>
<li>视频可以利用相邻帧的信息进行修复，但是如果单纯借用相邻帧的信息而不加操作的话，由于存在运动物体，可能在相邻两帧之间运动的位移比较大，就会使修复后的视频帧出现不对齐或伪影。论文的核心之处在于借用相邻帧的信息时，使用光流将运动物体扭曲到目标帧的正确空间位置，从而避免了上述问题。</li>
<li>流有效性mask：在遮挡和快速运动的条件下，上述扭曲操作是不准确的，会对生成的视频帧的结果造成影响，因此计算了流有效性mask，在使用相邻帧的信息时，仅使用有效的像素。</li>
</ul>
<p>②FuseFormer Fusing Fine-Grained Information</p>
<p>该论文创新之处有以下两点：</p>
<ul>
<li>为了解决由硬patch分割而出现的patch边缘细节模糊的问题而引入了软分割和软合成操作。</li>
<li>FuseFormer：<ul>
<li>在MLP之间加入软分割和软合成操作，没有想明白为什么要在这里添加这两个操作，但是个人猜测可能是添加这两个操作之后的实验效果更好吧、、、</li>
<li>为了使中间特征向量能够生成二维特征图更改了MLP的输入输出通道。</li>
</ul>
</li>
</ul>
</li>
<li><h5 id="video-matting：Video-Matting-via-Consistency-Regularized-Graph-Neural-Networks"><a href="#video-matting：Video-Matting-via-Consistency-Regularized-Graph-Neural-Networks" class="headerlink" title="video matting：Video Matting via Consistency-Regularized Graph Neural Networks"></a>video matting：Video Matting via Consistency-Regularized Graph Neural Networks</h5><ul>
<li><p>合成视频抠图：为了能生成时间一致增强的视频抠图结果，其中，时间一致性是帧与帧之间的关系，因此其使用GNN来完成帧与帧之间的连接。</p>
<ul>
<li>特征聚合：使用可变形卷积做的，目的是为了解决由于目标的运动而造成的特征不对齐问题。</li>
<li>节点状态更新：为什么要做这个更新没有很理解</li>
<li>网络预测：生成抠图结果</li>
</ul>
</li>
<li><p>真实视频抠图：上述操作在合成数据集上效果很好，需要将其泛化到真实数据集上</p>
<ul>
<li>一致性正则化：为了能更好地泛化到真实数据集</li>
<li>对抗性学习：为了解决域gap</li>
</ul>
</li>
</ul>
</li>
<li><h5 id="video-editing：A-Latent-Transformer-for-Disentangled-Face-Editing-in-Images-and-Videos"><a href="#video-editing：A-Latent-Transformer-for-Disentangled-Face-Editing-in-Images-and-Videos" class="headerlink" title="video editing：A Latent Transformer for Disentangled Face Editing in Images and Videos"></a>video editing：A Latent Transformer for Disentangled Face Editing in Images and Videos</h5><ul>
<li>预处理：提取人脸图像并对齐，以在StyleGAN的隐空间编辑视频帧（人脸部分）。</li>
<li>图像编辑：最核心的部分，利用StyleGAN来实现属性的编辑。</li>
<li>无缝克隆：将编辑过的人脸部分混合回原来的视频帧，达到编辑视频中人脸的效果。</li>
</ul>
</li>
<li><h5 id="video-generation：Depth-Aware-Generative-Adversarial-Network-for-Talking-Head-Video-Generation"><a href="#video-generation：Depth-Aware-Generative-Adversarial-Network-for-Talking-Head-Video-Generation" class="headerlink" title="video generation：Depth-Aware Generative Adversarial Network for Talking Head Video Generation"></a>video generation：Depth-Aware Generative Adversarial Network for Talking Head Video Generation</h5><p>这一篇论文的任务是让原图像生成和驱动视频一样的说话视频，这篇论文的pipeline共有三部分，我认为它们之间是环环相扣的，除第三步外，每一步都是在为下一步服务。</p>
<ul>
<li>自监督学习深度：为了下一步能够利用深度图来检测关键点。</li>
<li>基于深度的面部关键点探测：为了在第三步中生成运动场，将驱动视频的面部运动迁移到原图像。</li>
<li>跨模态注意力机制：将运动和外观融合，生成运动场以扭曲原图像的特征图。利用扭曲的特征图，生成说话视频。<ul>
<li>利用深度图和扭曲特征学习了深度感知注意力图：目的是细化扭曲特征，使其更关注面部的表情细节。</li>
</ul>
</li>
</ul>
</li>
<li><h5 id="video-restoration：-Restore-from-Restored-Video-Restoration-with-Pseudo-Clean-Video"><a href="#video-restoration：-Restore-from-Restored-Video-Restoration-with-Pseudo-Clean-Video" class="headerlink" title="video restoration： Restore from Restored: Video Restoration with Pseudo Clean Video"></a>video restoration： Restore from Restored: Video Restoration with Pseudo Clean Video</h5><p> 只是提供了一种新的方法——从恢复中恢复，该方法对于视频的restoration不需要GT和光流估计，但是仅限于FCN。基本思路是用一个预训练的网络对于带噪图像生成一个伪GT，在此伪GT上随机添加噪声，再把噪声去掉，损失函数为伪GT和最后去掉噪声版本的视频帧的L1或L2loss。</p>
</li>
</ul>
<h3 id="其他方向："><a href="#其他方向：" class="headerlink" title="其他方向："></a>其他方向：</h3><ul>
<li><h5 id="3D-inpainting：3D-Photography-using-Context-aware-Layered-Depth-Inpainting"><a href="#3D-inpainting：3D-Photography-using-Context-aware-Layered-Depth-Inpainting" class="headerlink" title="3D inpainting：3D Photography using Context-aware Layered Depth Inpainting"></a>3D inpainting：3D Photography using Context-aware Layered Depth Inpainting</h5><ul>
<li>初始化及深度图清洗： 目标是修补场景中被遮挡的部分，因此我们需要找到深度不连续的地方，因为这些地方需要扩展现有内容。</li>
<li>确定上下文合成区域：根据上下文确定inpainting范围</li>
<li>上下文感知的颜色和深度修复模型：3个子网络，先修复边，再根据边修复颜色和深度，最后转为3D图。用边引导颜色和深度，更加合理准确。</li>
</ul>
</li>
<li><h5 id="semantic-view-synthesis"><a href="#semantic-view-synthesis" class="headerlink" title="semantic view synthesis"></a>semantic view synthesis</h5><ul>
<li>跨模态蒸馏：生成[语义图像，深度]训练对</li>
<li>语义深度和图像合成：训练深度和颜色流</li>
<li>MPI预测：预测MPI以生成新视角的图像</li>
</ul>
<p>发现一些小小的共性</p>
<ol>
<li><p>Robust Consistent Video Depth Estimation和Progressive Temporal Feature Alignment Network for Video Inpainting均使用了流有效性mask，在光流扭曲的过程中，可能会出现运动过快导致光流估计不准确，或遮挡导致的帧i的像素在帧j中匹配不到，使用流有效性mask可以改善这些问题。</p>
</li>
<li><p>有多篇文章都在解决由于目标的运动而造成的不对齐问题，方法有以下几种：</p>
<ul>
<li>使用双线性插值</li>
<li>使用光流扭曲</li>
<li>使用可变形卷积</li>
<li>关键点探测生成运动场扭曲特征</li>
</ul>
<p>可能有些因果关系盘得比较肤浅，但是至少知道了如果遇到类似的问题或者目的，这些方法是可以借鉴的。</p>
</li>
</ol>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/12/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%9B%98/" data-id="cl3ixizbw0001not5hyzo1el3" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-summary" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/09/summary/" class="article-date">
  <time class="dt-published" datetime="2022-04-09T12:26:42.157Z" itemprop="datePublished">2022-04-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="看过的论文及周报汇总"><a href="#看过的论文及周报汇总" class="headerlink" title="看过的论文及周报汇总"></a>看过的论文及周报汇总</h1><h4 id="论文汇总"><a href="#论文汇总" class="headerlink" title="论文汇总"></a>论文汇总</h4><ol>
<li><a target="_blank" rel="noopener" href="https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/3DPhoto_supp.pdf">3D Photography using Context-aware Layered Depth Inpainting</a> </li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.05901.pdf">Robust Consistent Video Depth Estimation </a> </li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.10598.pdf">Semantic View Synthesis</a> </li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Lee_Restore_From_Restored_Video_Restoration_With_Pseudo_Clean_Video_CVPR_2021_paper.pdf">Restore from Restored: Video Restoration with Pseudo Clean Video</a> </li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zou_Progressive_Temporal_Feature_Alignment_Network_for_Video_Inpainting_CVPR_2021_paper.pdf">Progressive Temporal Feature Alignment Network for Video Inpainting</a> </li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Video_Matting_via_Consistency-Regularized_Graph_Neural_Networks_ICCV_2021_paper.pdf">Video Matting via Consistency-Regularized Graph Neural Networks</a> </li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yao_A_Latent_Transformer_for_Disentangled_Face_Editing_in_Images_and_ICCV_2021_paper.pdf">A Latent Transformer for Disentangled Face Editing in Images and Videos</a> </li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_FuseFormer_Fusing_Fine-Grained_Information_in_Transformers_for_Video_Inpainting_ICCV_2021_paper.pdf">FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting</a> </li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.06605v1">Depth-Aware Generative Adversarial Network for Talking Head Video Generation</a> </li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.09457">Look Outside the Room: Synthesizing A Consistent Long-Term 3D Scene Video from A Single Image</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.02573v1">How Me What and Tell Me How: Video Synthesis via Multimodal Conditioning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.09514v1">Mocycle-GAN: Unpaired Video-to-Video Translation  Mocycle GAN</a></li>
<li><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Liu_Liquid_Warping_GAN_A_Unified_Framework_for_Human_Motion_Imitation_ICCV_2019_paper.pdf">Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis</a> </li>
<li><a target="_blank" rel="noopener" href="https://cv-rits.github.io/MonoScene/">MonoScene: Monocular 3D Semantic Scene Completion (cv-rits.github.io)</a> </li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.05892.pdf">PixelSynth: Generating a 3D-Consistent Experience from a Single Image</a></li>
</ol>
<h4 id="周报汇总"><a href="#周报汇总" class="headerlink" title="周报汇总"></a>周报汇总</h4><ol>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/1uzpNsf_Ba">https://www.mubucm.com/doc/1uzpNsf_Ba</a> 论文2、3</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/A3tIvTsA0a">https://www.mubucm.com/doc/A3tIvTsA0a</a> 论文4、5</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/duTMi9vCPG">https://www.mubucm.com/doc/duTMi9vCPG</a> 论文5</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/M4uRYbZJcG">https://www.mubucm.com/doc/M4uRYbZJcG</a> 论文6、7</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/robDbUd9ai">https://www.mubucm.com/doc/robDbUd9ai</a> 论文8、9</li>
<li><a target="_blank" rel="noopener" href="https://honeypot1207.github.io/2022/04/12/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%9B%98/">0209lucky的望远镜 (honeypot1207.github.io)</a> 论文复盘</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/1eWUCjGGHO">https://www.mubucm.com/doc/1eWUCjGGHO</a> 论文10、11</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/ymz1KNbx3i">https://www.mubucm.com/doc/ymz1KNbx3i</a> 论文12</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/D26bhaSzKW">https://www.mubucm.com/doc/D26bhaSzKW</a> 论文13</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/W3yH-44Tq">https://www.mubucm.com/doc/W3yH-44Tq</a> 论文14</li>
<li><a target="_blank" rel="noopener" href="https://www.mubucm.com/doc/B8CRioCWvW">https://www.mubucm.com/doc/B8CRioCWvW</a> 论文15</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/04/09/summary/" data-id="cl3ixizbh0000not51go3g76h" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/12/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%9B%98/">(no title)</a>
          </li>
        
          <li>
            <a href="/2022/04/09/summary/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 Xiaolin Liu<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>